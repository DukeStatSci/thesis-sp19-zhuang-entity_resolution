[
["index.html", "Entity Resolution with an Application to the El Salvadoran Conflict Abstract", " Entity Resolution with an Application to the El Salvadoran Conflict Bihan Zhuang April 2019 Abstract Entity resolution (record linkage or de-duplication) is the process of removing duplicate entities in large, noisy databases. Entity resolution is made even more difficult when unique identifiers are not present and many of the observed records are subject to missing values. Furthermore, entity resolution has tradeoffs regarding assumptions of the data generation process, error rates, and computational scalability that make it a difficult task for real applications. In this paper, we are motivated to study a real data set from El Salvador, where a Truth Commission formed by the United Nations in 1992 collected data on killings that occurred during the Salvadoran civil war (1980-1991). Due to the data collection process, victims can be duplicated, as they may have been reported by different relatives, friends, or grass roots teams working in the area. Our motivation is to be able (1) to build flexible and robust models that are computationally fast, (2) to better understand what types of models are well suited for conflict data, (3) and finally provide estimates and evaluations of the number of documented identifiable deaths for our motivating data set. Keywords: record linkage, entity resolution, de-duplication, conflict data, Bayesian methods, El Salvador "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements I would like to thank… "],
["introduction.html", "Chapter 1 Introduction 1.1 The United Nations Truth Commission for El Salvador", " Chapter 1 Introduction Very often information about social entities is scattered across multiple databases. Combining that information into one database can result in enormous benefits for analysis, resulting in richer and more reliable conclusions. In most practical applications, however, analysts cannot simply link records across databases based on unique identifiers, such as social security numbers, either because they are not a part of some databases or are not available due to privacy concerns. In such cases, analysts need to use methods from statistical and computational science known as entity resolution (also called record linkage or de-duplication) to proceed with analysis. Entity resolution (ER) is not only a crucial task for social science and industrial applications, but is a challenging statistical and computational problem itself, because many databases contain errors (noise, lies, omissions, duplications, etc.), and the number of parameters to be estimated grows with the number of records . To meet present and near-future needs, entity resolution methods must be flexible and scalable to large databases; furthermore, they must be able to handle uncertainty and be easily integrated with post-linkage statistical analyses, such as logistic regression or capture recapture. All this must be done while maintaining accuracy and low error rates. Turning to the context of an armed conflict, creating such models is incredibly challenging as typically grass roots movements, families, friends collect multiple reports on the same victims. This naturally causes duplications to occur in the data. In this paper, we study a real example from El Salvador, where a Truth Commission was formed by the United Nations in 1992. This Truth Commission collected data on killings that occurred during the Salvadoran civil war (1980 – 1991). Given the data collection process, a victim can be reported by different family and friends, and thus, one important aspect is to remove any duplications in the data in order to make it more reliable. In addition, removing such duplications allows one to estimate the number of documented identifiable deaths. 1.1 The United Nations Truth Commission for El Salvador Between 1980 and 1991, the Republic of El Salvador witnessed a civil war between the central government, the left-wing guerrilla Farabundo Mart National Liberation Front (FMLN), and the right-wing para-military death squads. After the peace agreement in 1992, the United Nations created a Commission on the Truth (UNTC) for El Salvador, which invited members of Salvadoran society to report war-related human rights violations, which mainly focused on killings and disappearances. In order to collect such information the UNTC invited individuals through newspaper, radio, and television advertisements to come forward and testify. The UNTC opened offices through El Salvador where witnesses could provide their testimonials, and this resulted in a list of potential victims with names, date of death, and reported location. Due to the fact that testimonials were provided to the UNTC many years after the civil war, it is expected that witnesses could not recall some of the details of the killings. In addition, some details regarding testimonials of the same individual, may contain conflicting or differing information. This is a natural characteristic of this data and leads to more noise, distortions, and missingness in the data. Furthermore, a victim can be reported multiple times, which leads to the an issue with duplication in the data. Finally, there is not unique identifiers for this data set that are thought to be reliable, which motivates the use of fully unsupervised Bayesian methods. Our work builds off the seminal work of , where we use the same data set for consistency. We refer to for complete details regarding the UNTC data set. The entire data set contains 5395 records. The fields (features) used in this paper are full name, date of death (year, month and day), municipality, and department. Table 1.1 provides an illustrative example of how duplicates can appears in the UNTC data set. Records 1, 2, and 3 in Table 1.1 represent an example of three duplicated records that most likely refer to the same person. This example is one example, where we may have non-coreferent decisions made by models or by humans in making decisions between pairs of records due to the nature of the data at hand. One advantage to our proposed approach is that we never look at pairwise comparisons of records. Turing to the second example in our table, records 3 and 4 agree on all the same information except on given name and family name. This illustrates potential issues that one faces regarding Hispanic names. For example, record 5 may refer to the same person in record 4. Record 5 could have typographical and missing information as it’s quite common for a given name of JULIAN ANDRES to drop a given name to JULIAN. Turning too the family name, It’s also quite common for one of the family names to be dropped. Thus, RAMOS ROJAS could be shortened to RAMOS. The typographical errors are quite common as the original data was scanned using OCR, and this is the most likely reason that such errors would appear. In short, declaring records 4 and 5 as co-referent depends highly we believe on the agreement or disagreement on the fields. Table 1.1: Illustrative example of duplicated records in the UNTC data set. Records 1 – 3 should refer to the same entity. Records 4 — 5 should refer to the same entity. Record Given name Family name Year Month Day Municipality 1. JOSE FLORES 1981 1 29 A 2. JOSE FLORES 1981 2 NA A 3. JOSE FLORES 1981 3 20 A 4. JULIAN ANDRES RAMOS ROJAS 1986 8 5 B 5. JILIAM RAMOS 1986 8 5 B "],
["priorworks.html", "Chapter 2 Current Approaches to Record Linkage 2.1 Overview of the Article", " Chapter 2 Current Approaches to Record Linkage Many modern record linkage techniques can be viewed as an extension of the Fellegi-Sunter approach (FS), which computes pairwise probabilities of matching for all pairs of records using a likelihood ratio test . While modern FS methods are used today, such implementations assume that only two databases can be linked and that there are no duplicates within each database . Furthermore, such approaches are known to be quite sensitive to the choice of the threshold that the likelihood ratio test is based upon. In short, these assumptions are inadequate for many record linkage tasks. Bayesian methods have been recently utilized in record linkage due to their flexibility and exact error propagation; however, they have been limited primarily to two-database matching, issues with scalability to large databases, and model misspecification . These contributions, while valuable, do not easily generalize to multiple databases and to duplication within databases. The most relevant work to our proposed methodology is that of , which deals with a special case of record linkage known as duplicate detection. Duplicate detection refers to removing duplicate entities within a data file (but not across and within data files). In , the authors propose a duplicate detection approach borrowing approaches from , the Bayesian literature, and the blocking literature. Blocking (filtering or indexing) is a way of reducing down the entire space of records, such that one only must compare similar records. first reduces down the space of all records using deterministic blocking rules. Next, the authors propose a Bayesian model based upon comparison data, which is an input from the blocking stage. One benefit of this is there is a computational cost from filtering records pairs, however, there is a tremendous drawback in that there is no way to propagate the uncertainty from the blocking mechanism in the duplicate detection step. The authors evaluate their proposed methodology for two municipalities, where ground truth is thought to be accurate. Further evaluations are performed in an entirely unsupervised fashion on the remaining municipalities. In a fully hierarchical-Bayesian approach to record linkage, using Dirichlet prior distributions over latent attributes and assuming a data distortion model. The authors derived an efficient hybrid (Metropolis-within-Gibbs) MCMC algorithm for fitting these models, SMERED. SMERED updates most of the latent variables and parameters using Gibbs sampling from conjugate conditional distributions. It updates the bipartite graph using a split-merge step, following . Thus, one has all the advantages of the Bayesian paradigm for both the latent entities and the linkage structure. Similar bipartite graph structures have been considered in the two-database scenario . The attributes of the latent entities, the number of latent entities, the edges linking records to latents, etc., all have posterior distributions, and it is easy to sample from these distributions for uncertainty quantification or error propagation. More recently, extended these approaches to both categorical and noisy string data using an empirically motivated prior, , which is available on . The authors illustrated on real and simulated data that the EB method beat supervised methods (e.g., random forests, Bayesian Adaptive Regression Trees, logistic regression) when the training data is 10 percent (or less) of the total amount of data. While SMERED and the EB method work on moderately sized data sets, there are potential limitations with scaling to industrial-sized data sets. For a review on recent developments in Bayesian methods, see . 2.1 Overview of the Article In this section, we provide an overview of the article. In this paper, we provide five contributions to the literature. First, we propose an extension to the model for end-to-end empirical Bayesian entity resolution . Specifically, we provide to our knowledge the first use of subjective priors on the linkage structure for generalized entity resolution. We consider two non-parametric priors on the linkage structure, which are the Pitman-Yor Process prior and the Dirichlet Process prior. Second, we do not require any dimension reduction (such as blocking) to be applied to the data, which means that the only sources of error in our inferential methods comes from the data and the entity resolution task. Third, our extension using generalized entity resolution propagates the error of the entity resolution task exactly into our inferential task. Fourth, our method considers an application the synthetic data, where we can understand and evaluate our methodology rigorously. Finally, our method looks at the case study to the UNTC data set from a fully unsupervised point of view. In Section 3, we review prior methodology that is used in this paper, followed by a description of our proposed methodology. In Section 4, we apply our proposed model to a synthetic data set. In Section 5, we test our proposed methodology on our motivational data set from El Salvadoran Conflict, and then provide a discussion regarding our proposed work and directions for future research. "],
["methodology.html", "Chapter 3 Methodology 3.1 Notation and Assumptions 3.2 Background on Empirical Bayesian Entity Resolution 3.3 Attribute Similarity Measures 3.4 Model Specification 3.5 Entity Resolution with the Bayesian Nonparametric Priors 3.6 Posterior Joint Distribution", " Chapter 3 Methodology In this section, we first give notation and assumptions that is used throughout the rest of the paper in Section 3.1. We then review prior work that we build upon in Section 3.2, before describing the attribute similarity measure in Section 3.3. In Section 3.4 we outline the proposed generative process of entity resolution. In Section 3.5 we describe the use of Bayesian nonparametric prior on the linkage structure. Finally, we provide the posterior distribution under our proposed model in Section 3.6. 3.1 Notation and Assumptions Let \\(i\\in\\{1,\\ldots,D\\}\\) index databases and \\(j\\in\\{1,\\ldots,R_i\\}\\) index records within each database. Allow \\(j&#39;\\in\\{1,\\ldots,N\\}\\) index true individuals, where \\(N=\\sum_{i=1}^D R_i\\) without loss of generality. Our indexing allows for categorical or string-field data. (For example, if the categorical data is thought to be reliable, we would like to avoid comparisons of gender. On the other hand, text-style data, such as name and address should be treated as strings.) Given this, let \\(\\ell\\in\\{1,\\ldots,p_s\\}\\) index string-valued fields, and let \\(\\ell\\in\\{p_s+1,\\ldots,p_s+p_c\\}\\) index categorical fields. Using the same notation as , \\(X_{ij\\ell}\\) denotes the observed value of the \\(\\ell\\)th field for the \\(j\\)th record in the \\(i\\)th database and it is assumed to be a noisy observation of \\(Y_{j&#39;\\ell}\\) denotes the true value of the \\(\\ell\\)th field for the \\(j&#39;\\)th latent individual. Additionally, we incorporate the possibility that some attributes \\(X_{ij\\ell}\\) may be missing at random through a corresponding observed indicator \\(O_{ij\\ell}\\). \\(O_{ijl} = 1\\) implies that \\(X_{ijl}\\) is observed and \\(O_{ijl} = 0\\) implies that \\(X_{ijl}\\) is missing. We also define \\(\\boldsymbol{X}^{obs} = \\{X_{ijl} : O_{ijl} = 1\\}\\) as the observed part and \\(\\boldsymbol{X}^{miss} = \\{X_{ijl} : O_{ijl} = 0\\}\\) as the missing part of \\(\\boldsymbol{X}\\). Let \\(\\lambda_{ij}\\) denote the assigned latent individual to which the \\(j\\)th record in the \\(i\\)th database corresponds, i.e., \\(X_{ij\\ell}\\) and \\(Y_{j&#39;\\ell}\\) represent the same individual if and only if \\(\\lambda_{ij}=j&#39;\\). Finally, allow the distortion parameter to be \\(z_{ij\\ell}=I(X_{ij\\ell}\\ne Y_{\\lambda_{ij}\\ell})\\). We next introduce notation for empirical distributions. For each \\(\\ell\\in\\{1,\\ldots,p_s+p_c\\}\\), let \\(S_\\ell\\) denote the set of all values for the \\(\\ell\\)th field that occur anywhere in the data, i.e., \\(S_\\ell=\\{X_{ij\\ell}:1\\le i\\le D, 1\\le j\\le R_i\\}.\\) Define \\(\\alpha_\\ell(v)=\\frac{1}{N}\\sum_{i=1}^D\\sum_{j=1}^{R_i}I(X_{ij\\ell}=v)\\) to be the relative frequency of \\(v\\) in data for field \\(\\ell\\). For each \\(\\ell\\in\\{1,\\ldots,p_s\\}\\) and all possible values \\(v\\in S_\\ell\\), let \\(F_\\ell(v)\\) denote the distribution defined as follows: If \\(W \\sim F_\\ell(v)\\), then for every \\(w\\in S_\\ell\\), \\[ P(W=w)=\\frac{\\alpha_\\ell(w)\\,\\exp\\!\\left[-c\\,d(v,w)\\right]}{\\sum_{w\\in S_\\ell}\\alpha_\\ell(w)\\,\\exp\\!\\left[-c\\,d(v,w)\\right]}\\propto\\alpha_\\ell(w)\\,\\exp\\!\\left[-c\\,d(v,w)\\right], \\] where \\(d(\\cdot,\\cdot)\\) is a string similarity measure and \\(c&gt;0\\). Remark: \\(F_{\\ell}\\) is used to choose values proportional to their empirical frequency, while placing more weight on those that are more “similar” to \\(w\\) in terms of the similarity measure. This intuitively says that the distorted values are likely to be close to the truth. A more detailed discussion about the string similarity measure can be found in Section 3.2. For each \\(\\ell\\in\\{1,\\ldots,p_s+p_c\\}\\), let \\(G_\\ell\\) denote the empirical distribution of the data in the \\(\\ell\\)th field from all records in all databases combined. In other words, if a random variable \\(W\\) has distribution \\(G_\\ell\\), then for every \\(w\\in S_\\ell\\), \\[ P(W=w)=\\alpha_\\ell(w). %\\frac{1}{N}\\sum_{i=1}^k\\sum_{j=1}^{n_i}I(X_{ij\\ell}=w)\\\\ %&amp;=\\text{relative frequency of $w$ in the data for field $\\ell$}. \\] Remark: \\(G_\\ell\\) depends on the values of \\(\\boldsymbol{X}\\). However, the idea is that we construct \\(G_\\ell\\) doing any computations with the model. So although \\(G_\\ell\\) “depends on” \\(\\boldsymbol{X}\\) when we construct it, we don’t treat \\(G_\\ell\\) as if it depends on \\(\\boldsymbol{X}\\) when we plug it into the model. We construct \\(G_\\ell\\) using \\(\\boldsymbol{X}\\), but then we “forget” where \\(G_\\ell\\) came from when we use it in the model. This is exactly the same thing that happens—conceptually—in any empirical Bayesian procedure. As usual, let \\(\\delta(v)\\) denote the distribution of a random variable that takes the value \\(v\\) with probability \\(1\\). A summary of notation is provided in Table 3.1. Table 3.1: Summary of notation Symbol Description Symbol Description \\(i \\in 1 \\ldots D\\) index over databases \\(y_{j&#39;\\ell}\\) attribute \\(\\ell\\) for entity \\(j&#39;\\) \\(j \\in 1 \\ldots R_i\\) index over records in db \\(i\\) \\(\\lambda_{ij}\\) assigned entity for record \\(j\\) in db \\(i\\) \\(j&#39; \\in 1 \\ldots N\\) index over true individuals \\(\\beta_{i\\ell}\\) prob. attribute \\(\\ell\\) in db \\(i\\) is distorted \\(\\ell \\in 1 \\ldots p_s + p_c\\) index over attributes \\(a_{\\ell}, b_{\\ell}\\) distortion hyperparams. for attribute \\(\\ell\\) \\(v\\in1\\ldots|\\mathcal{S}_{\\ell}|\\) index over domain of attribute \\(\\ell\\) \\(\\vartheta, \\sigma\\) BNP hyperparams. for clustering \\(\\mathcal{S}_{\\ell}\\) domain of attribute \\(\\ell\\) \\(R = \\sum_{i} R_i\\) total number of records \\(\\alpha_{\\ell}(\\cdot)\\) distribution over domain of attribute \\(\\ell\\) \\(\\mathtt{sim}_{\\ell}(\\cdot, \\cdot)\\) similarity measure for attribute \\(\\ell\\) \\(X_{ij\\ell}\\) attribute \\(\\ell\\) for record \\(j\\) in table \\(i\\) \\(z_{ij\\ell}\\) distortion indicator for \\(X_{ij\\ell}\\) \\(O_{ij\\ell}\\) observed indicator for \\(X_{ij\\ell}\\) 3.2 Background on Empirical Bayesian Entity Resolution We review the end-to-end entity resolution framework of that lays the foundation. Assuming the notation defined above, the generative model can be written as: \\[\\begin{align*} X_{ij\\ell}\\mid \\lambda_{ij},\\,Y_{\\lambda_{ij}\\ell},\\,z_{ij\\ell}\\;&amp;\\sim\\begin{cases}\\delta(Y_{\\lambda_{ij}\\ell})&amp;\\text{ if }z_{ij\\ell}=0\\\\F_\\ell(Y_{\\lambda_{ij}\\ell})&amp;\\text{ if }z_{ij\\ell}=1\\text{ and }\\ell\\le p_s\\\\G_\\ell&amp;\\text{ if }z_{ij\\ell}=1\\text{ and }\\ell&gt;p_s\\end{cases}\\\\ %&amp;\\qquad\\text{for each }i\\in\\{1,\\ldots,k\\},\\; j\\in\\{1,\\ldots,n_i\\},\\; \\ell\\in\\{1,\\ldots,p_s+p_c\\},\\\\ %&amp;\\qquad\\text{with everything independent},\\\\ Y_{j&#39;\\ell}\\;&amp;\\sim G_\\ell\\\\ z_{ij\\ell}\\mid\\beta_{i\\ell}\\;&amp;\\sim \\text{Bernoulli}(\\beta_{i\\ell})\\\\ \\beta_{i\\ell}\\;&amp;\\sim\\text{Beta}(a,b)\\\\ \\lambda_{ij}\\;&amp;\\sim\\text{DiscreteUniform}(1,\\ldots,N) \\end{align*}\\] with everything independent of everything else. Since duplication is allowed within databases, any record can correspond to any latent individual. Hence, we can specify the prior the linkage structure by specifying it independently for each \\(\\lambda_{ij}\\) as shown above. We now give the joint posterior and full conditionals. For each \\(v\\in S_\\ell\\), define \\[ h_\\ell(v)=\\left\\{\\sum_{w\\in S_\\ell}\\exp\\!\\left[-c\\,d(v,w)\\right]\\right\\}^{-1}, \\] i.e., \\(h_\\ell(v)\\) is the normalizing constant for the distribution \\(F_\\ell(v)\\). We can compute \\(h_\\ell(v)\\) in advance for each possible \\(v\\in S_\\ell\\). After some simplification, the joint posterior of becomes (where the full conditional distributions are derived in Appendix ): \\[\\begin{align*} &amp;\\pi(\\pmb{\\Lambda},\\boldsymbol{Y},\\boldsymbol{z},\\boldsymbol{\\beta}\\mid \\boldsymbol{X})\\\\ %&amp;\\propto %\\prod_{i=1}^k\\prod_{j=1}^{n_i}\\left( %\\left\\{\\mathop{\\prod_{\\ell=1}^{p_s+p_c}}_{z_{ij\\ell}=0}I(X_{ij\\ell}=Y_{\\lambda_{ij}\\ell})\\right\\}\\left\\{\\mathop{\\prod_{\\ell=1}^{p_s+p_c}}_{z_{ij\\ell}=1}\\alpha_\\ell(X_{ij\\ell})\\right\\}\\left\\{\\mathop{\\prod_{\\ell=1}^{p_s}}_{z_{ij\\ell}=1}h_\\ell(Y_{\\lambda_{ij}\\ell})\\right\\}\\right.\\\\ %&amp;\\qquad\\qquad\\qquad\\left.\\times\\exp\\!\\left[-c %\\sum_{\\ell=1}^{p_s}z_{ij\\ell}\\, %d(X_{ij\\ell},Y_{\\lambda_{ij}\\ell})\\right] %\\right)\\\\ %&amp;\\qquad\\times\\left[\\prod_{j&#39;=1}^N\\prod_{\\ell=1}^{p_s+p_c}\\alpha_\\ell(Y_{j&#39;\\ell})\\right]\\left[\\prod_{i=1}^k\\prod_{\\ell=1}^{p_s+p_c}\\beta_{i\\ell}^{\\sum_{j=1}^{n_i}z_{ij\\ell}+a-1}(1-\\beta_{i\\ell})^{n_i-\\sum_{j=1}^{n_i}z_{ij\\ell}+b-1}\\right]\\\\ &amp;\\propto \\prod_{i=1}^D\\prod_{j=1}^{R_i}\\left\\{ \\left[\\mathop{\\prod_{\\ell=1}^{p_s+p_c}}_{z_{ij\\ell}=1}\\alpha_\\ell(X_{ij\\ell})\\right]\\left[\\mathop{\\prod_{\\ell=1}^{p_s}}_{z_{ij\\ell}=1}h_\\ell(Y_{\\lambda_{ij}\\ell})\\right]\\exp\\!\\left[-c \\sum_{\\ell=1}^{p_s}z_{ij\\ell}\\, d(X_{ij\\ell},Y_{\\lambda_{ij}\\ell})\\right] \\right\\}\\\\ &amp;\\qquad\\times\\left[\\prod_{j&#39;=1}^N\\prod_{\\ell=1}^{p_s+p_c}\\alpha_\\ell(Y_{j&#39;\\ell})\\right]\\left[\\prod_{i=1}^D\\prod_{\\ell=1}^{p_s+p_c}\\beta_{i\\ell}^{\\sum_{j=1}^{n_i}z_{ij\\ell}+a-1}(1-\\beta_{i\\ell})^{n_i-\\sum_{j=1}^{n_i}z_{ij\\ell}+b-1}\\right]\\\\ &amp;\\qquad\\times I(X_{ij\\ell}=Y_{\\lambda_{ij}\\ell}\\text{ for all }i,j,\\ell\\text{ such that }z_{ij\\ell}=0). \\end{align*}\\] 3.3 Attribute Similarity Measures In this section, we review the attribute similarity measures defined by . Definition 3.1 (Attribute similarity measure) Let \\(\\mathcal{V}\\) be the domain of an attribute. An on \\(\\mathcal{V}\\) is a function \\(\\mathtt{sim}: \\mathcal{V}\\times \\mathcal{V}\\to [0, s_\\mathrm{max}]\\) that satisfies \\(0 \\leq s_\\mathrm{max} &lt; \\infty\\) and \\(\\mathtt{sim}(v,w) = \\mathtt{sim}(w,v)\\) for all \\(v, w \\in \\mathcal{V}\\). These similarity measures is used to quantify the likelihood that some value \\(v\\) in the empirical distribution gets chosen as a distortion of the true value \\(w\\). Although the parameterization of attribute similarity is different from the distance measure of , proved that the two parameterization is in fact equivalent, as long as the distance measure is bounded and symmetric. We refer the readers to for detailed proofs of this result. During the process of inference, these similarities for the attributes may be expensive to evaluate on-the-fly, so consider caching and truncation of attribute similarities. Only similarities for pairs of values that fall above a cut-off \\(S_{cut;\\ell}\\) are being stored. This is achieved through the following truncation transformation to the raw attribute similarity \\(\\mathtt{sim}_{\\ell}(v,w)\\): \\[\\begin{equation} \\underline{\\mathtt{sim}}_{\\ell}(v,w) = \\max \\left(0, \\ \\frac{\\mathtt{sim}_{\\ell}(v,w) - s_{\\mathrm{cut};\\ell}} {1 - s_{\\mathrm{cut};\\ell}/s_{\\mathrm{max};\\ell}} \\right). \\end{equation}\\] Pairs of values not present in the cache have a truncated similarity of zero by default. We refer readers to Section 6.2 of the paper for discussions about this efficiency consideration. In this section we also discuss what the appropriate distance functions for the UNTC data would be like. Distances such as the Levenshtein distance would perform poorly because of situations like a dropped name or a re-ordered name would imply a large edit distance. Instead, we consider the Monge-Elkan distance of , a hybrid similarity measure, that seems more appropriate as it is insensitive to the variations above. We define Monge-Elkan distance as \\[\\begin{equation} \\texttt{sim}_{\\ell}^{\\text{M-E}}(A,B) = \\frac{1}{|A|} \\sum_{a \\in A} \\max_{b \\in B} \\texttt{sim}_{\\ell}^{\\prime}(a,b) \\end{equation}\\] where \\(A,B\\) are attributes with several words (e.g. JOSE TITO), \\(a \\in A, b\\in B\\) are words in an attribute (e.g. JOSE), and \\(\\texttt{sim}^{\\prime}\\) is a base similarity measure (such as the normalized edit similarity). With this formulation, we thus compare average similarity between all existing words in the two attributes, ignoring the ordering of them. We then define the asymmetric similarity function that we use for the string attributes in the UNTC data: \\[ \\texttt{sim}_{\\ell}(A,B) = \\begin{cases} 0 &amp;\\mbox{if } |A| &lt; |B| \\\\ \\texttt{sim}_{\\ell}^{\\text{M-E}}(A,B) &amp; \\text{otherwise.} \\end{cases} \\] If attribute \\(A\\) contains fewer words than attribute \\(B\\), then we immediately treat \\(A\\) as not similar to \\(B\\). Otherwise we use the Monge-Elkan distance to measure their similarity. 3.4 Model Specification We now describe the generative process of our proposed model. The model assumes a total of \\(N\\) latent entities whose attributes have the true values. The value of attribute \\(\\ell\\) from the \\(j&#39;\\) latent entity is to be drawn independently from the empirical distribution: \\[ Y_{j&#39;\\ell} \\sim G_{\\ell}. \\] We draw a distortion probability for each attribute \\(\\ell\\) in database \\(i\\) assuming \\[ \\beta_{i\\ell} | a_{\\ell}, b_{\\ell} \\sim Beta(a_{\\ell}, b_{\\ell}), \\] where \\(a_{\\ell}, b_{\\ell}\\) are hyperparameters that we tune. We assume the records are generated one after another in an iterative fashion. Different from , we no longer do so by selecting a latent entity uniformly at random. Instead, we incorporate subjective, more flexible priors on the linkage structure \\(\\Lambda\\). The generative process is described below. Draw a latent entity assignment from a Bayesian nonparametric prior. Specifically, we consider the Pitman Yor Process prior and the Dirichlet Process prior (generalized as BNP Prior here): \\[ \\lambda_{ij} \\sim \\text{BNP Prior}(\\vartheta, \\sigma), \\] where \\(\\vartheta\\) and \\(\\sigma\\) are the hyperparameters of these two BNP priors. We provide details about the two priors in Section . For attribute \\(\\ell\\) of record \\(j\\) in database \\(i\\), draw a distortion indicator \\(z_{ij\\ell}\\): \\[ z_{ij\\ell} | \\beta_{i\\ell} \\sim Bernoulli(\\beta_{i\\ell}). \\] Draw the record value \\(X_{ij\\ell}\\) from a hit-or-miss model (different from the model of , we also incorporate attribute similarity measures to categorical fields): \\[ X_{ij\\ell} | \\lambda_{ij},\\,Y_{\\lambda_{ij}\\ell},\\,z_{ij\\ell} \\sim (1 - z_{ij\\ell}) \\delta(Y_{\\lambda_{ij}\\ell}) + z_{ij\\ell} \\phi(X_{ij\\ell} | Y_{\\lambda_{ij}\\ell}), \\] where \\[ \\phi(X_{ij\\ell} = w| Y_{\\lambda_{ij}\\ell}) = \\frac{\\alpha_\\ell(w)\\,\\exp\\!\\left[-c\\,d(w,Y_{\\lambda_{ij}\\ell})\\right]}{\\sum_{w\\in S_\\ell}\\alpha_\\ell(w)\\,\\exp\\!\\left[-c\\,d(w,Y_{\\lambda_{ij}\\ell})\\right]}\\propto\\alpha_\\ell(w)\\,\\exp\\!\\left[-c\\,d(w,Y_{\\lambda_{ij}\\ell})\\right] \\] for all attributes string-valued and categorical. If \\(z_{ij\\ell}=0\\) or no distortion, then the value of \\(X_{ij\\ell}\\) is exactly that of the corresponding latent entity. If \\(z_{ij\\ell}=1\\) or distortion, \\(X_{ij\\ell}\\) is then drawn from a weighted empirical distribution, with similarity measures. Equivalently, given the result of , we can write \\[ \\phi(X_{ij\\ell} = w| Y_{\\lambda_{ij}\\ell}) = \\frac{\\alpha_\\ell(w)\\,\\exp\\!\\left[\\mathtt{sim}_{\\ell}(w, Y_{\\lambda_{ij}\\ell})\\right]}{\\sum_{w\\in S_\\ell}\\alpha_\\ell(w)\\,\\exp\\!\\left[\\mathtt{sim}_{\\ell}(w, Y_{\\lambda_{ij}\\ell})\\right]}\\propto\\alpha_\\ell(w)\\,\\exp\\!\\left[\\mathtt{sim}_{\\ell}(w, Y_{\\lambda_{ij}\\ell})\\right]. \\] 3.5 Entity Resolution with the Bayesian Nonparametric Priors The empirical Bayesian approach (EB) of uses a uniform prior to model the prior distribution of the linkage structure \\(\\boldsymbol{\\Lambda}\\). The uniform prior assumes that every legitimate configuration of the \\(\\lambda_{ij}\\) is equally likely a priori, and this implies a default prior on related quantities, such as the number of individuals in the data . Moreover, each record is assumed to be equally likely to correspond to any of the \\(N\\) possible latent individuals a priori. While the choice of uniform prior is convenient and simplifies the computation of the posterior, there are several weaknesses that should be addressed. Firstly, the uniform prior is constructed under the assumption that the \\(N\\) total records are randomly sampled with replacement from a population of \\(N\\) total latent individuals. This turns out to be quite a strong assumption on the linkage structure. We assume that the total number of latent individuals has the same size as the sample. We also restrict the latent population size to be maximum \\(N\\), and not considering the case that the latent population size being greater than \\(N\\). Secondly, showed that even though the uniform prior is often regarded as an non-informative prior, it is actually highly informative in the EB model because under certain conditions the data will not be able to overwhelm the prior, which defeats the purpose of developing a Bayesian model. For the above reasons, we consider more well-principled, subjective priors for the linkage prior. More specifically, we consider the Pitman-Yor prior (PYP). We first present notation that is used throughout the remainder of the paper and then derive the full conditional distributions. In terms of inference, we implement a standard Gibbs sampler. (We also consider the case of the Dirichlet process prior in our experiments given that this prior is a special case of the PYP). The PYP prior is adapted from (Pitman, 2006). Assume a total of \\(D\\) databases. Assume that \\(\\lambda_{1,1}, ..., \\lambda_{i,j-1}\\) are already classified into \\(k_{i,j-1}\\) clusters identified by the population labels \\(j&#39;_1, ..., j&#39;_{k_{i,j-1}}\\). The clusters have sizes \\(n_1, ..., n_{k_{i,j-1}}\\) respectively. In our context this means \\(\\lambda\\)’s that belong to the same cluster the same latent entity. Let \\(N_{i,j-1}\\) denote the total number of these records. We now consider the classification of \\(\\lambda_{ij}\\), the label of the latent individual to which the \\(j\\)th record in the \\(i\\)th database corresponds. Recall that the PYP has three parameters, a parameter \\(\\vartheta\\), a parameter \\(\\sigma\\), and a base distribution \\(H_0\\). Under the PYP prior, \\(\\lambda_{ij}\\) will either identify a new cluster with probability \\[\\begin{equation*} P(\\lambda_{ij} \\sim H_0 | \\lambda_{1,1}, ..., \\lambda_{i, j-1}, \\vartheta, \\sigma, H_0) = \\frac{k_{i,j-1}\\sigma + \\vartheta}{N_{i,j-1} + \\vartheta}, \\end{equation*}\\] or identify with an existing cluster with probability \\[\\begin{equation*} P(\\lambda_{ij} = j&#39;_g \\in \\{j&#39;_1, ..., j&#39;_{k_{i,j-1}}\\}| \\lambda_{1,1}, ..., \\lambda_{i, j-1}, \\vartheta, \\sigma, H_0) = \\frac{n_g - \\sigma}{N_{i,j-1} + \\vartheta}, \\end{equation*}\\] where the admissible values for the parameters are \\(\\sigma \\in [0,1)\\) with \\(\\vartheta &gt; -\\sigma\\) or \\(\\sigma &lt; 0\\) with \\(\\vartheta = m|\\sigma|\\) for some positive integer \\(m\\). Together \\(\\vartheta\\) and \\(\\sigma\\) control the formation of new cluster. The discount parameter \\(\\sigma\\) reduces the probability of adding a new record into the existing cluster. The PYP prior yields power-law behavior in terms of cluster behavior when \\(0 &lt; \\sigma &lt; 1\\). In addition, there is an obvious characteristic of the PYP prior, which is that the probability of a new record joining an existing cluster is proportional to the size of that cluster. So new records are more likely to join existing large clusters rather than a new cluster. This is often referred to as the “rich-get-richer” characteristic (Wallach, 2010). Note that under the PYP framework, we allow the latent population size to be greater than \\(N\\), which will be more applicable to real world scenarios. In addition, the results of this process are exchangeable, meaning the order in which the \\(\\lambda\\)’s identify with the clusters does not affect the probability of the final distribution, which is a desirable property of non-uniform priors. The above probabilities induce a prior on the set of all possible partitions of the \\(N\\) records which is \\[\\begin{equation*} P(Z(\\lambda) = z) = \\frac{(\\vartheta+\\sigma)_{k-1, \\sigma}}{(\\vartheta+1)_{N-1,1}} \\prod^{k}_{g=1} (1-\\sigma)_{n_g - 1, 1}, \\end{equation*}\\] where \\(\\{n_1, ...n_k \\}\\) are the cluster sizes of a particular partition \\(z\\), and \\(x_{r,s} = x(x+s)...(x+(r-1)s)\\) (Pitman, 2006). It can also be proved that under this prior setup, the expected value of the number of clusters in partition \\(z\\), \\(k(z)\\), is \\[\\begin{equation} \\label{eqn:1} E(k(z)) = \\sum^{N}_{i=1} \\frac{(\\vartheta+\\sigma)_{(i-1)\\uparrow}} {(\\vartheta+1)_{(i-1)\\uparrow}} = \\frac{\\vartheta}{\\sigma} \\Bigg[ \\frac{(\\vartheta+\\sigma)_{N \\uparrow}}{(\\vartheta)_{N \\uparrow}} -1 \\Bigg] \\end{equation}\\] and the variance is \\[\\begin{equation} \\label{eqn:2} Var(k(z)) = \\frac{\\vartheta (\\vartheta+\\sigma)}{\\sigma^2} \\frac{(\\vartheta+2\\sigma)_{N\\uparrow}}{(\\vartheta)_{N\\uparrow}} - \\frac{\\vartheta^2}{\\sigma^2} \\Bigg(\\frac{(\\vartheta+\\sigma)_{N \\uparrow}}{(\\vartheta)_{N \\uparrow}} \\Bigg)^2 - \\frac{\\vartheta}{\\sigma} \\frac{(\\vartheta+\\sigma)_{N \\uparrow}}{(\\vartheta)_{N \\uparrow}} \\end{equation}\\] with \\(x_{s\\uparrow} = \\Gamma(x+s) / \\Gamma(x)\\). We use the equations of expectation and variance for prior elicitation by selecting \\(\\vartheta\\) and \\(\\sigma\\) to have \\(E(k(z))\\) equal to a rough prior guess of the number of clusters and \\(Var(k(z))\\) equal to a specific amount of prior variability in the number of clusters. The Dirichlet Process (DP) is a special case of the Pitman-Yor Process when the discount parameter \\(\\sigma = 0\\). Recall the definition of Dirichlet Process: \\[ G \\sim DP(\\vartheta, H_0) \\] if for any partition \\((A_1, ..., A_k)\\) of \\(I{X}\\): \\[ \\left( G(A_1), ..., G(A_k) \\right) \\sim \\text{Dirichlet} \\left( \\vartheta H_0(A_1), ..., \\vartheta H_0(A_k) \\right) \\] where \\(H_0\\) is the base distribution, and \\(\\vartheta\\) is the concentration parameter. Under a DP prior, similar to the PYP prior, the predictive probability of cluster membership of all records constructs a partition of these records sequentially. Under the DP prior, \\(\\lambda_{ij}\\) will either identify a new cluster with probability (Wallach, 2010). \\[\\begin{equation*} P(\\lambda_{ij} \\sim H_0 | \\lambda_{1,1}, ..., \\lambda_{i, j-1}, \\vartheta, H_0) = \\frac{\\vartheta}{N_{i,j-1} + \\vartheta}, \\end{equation*}\\] or identify with an existing cluster with probability \\[\\begin{equation*} P(\\lambda_{ij} = j&#39;_g \\in \\{j&#39;_1, ..., j&#39;_{k_{i,j-1}}\\}| \\lambda_{1,1}, ..., \\lambda_{i, j-1}, \\vartheta, H_0) = \\frac{n_g}{N_{i,j-1} + \\vartheta}. \\end{equation*}\\] 3.6 Posterior Joint Distribution For the context of our problem, we will assume that the data is missing at random (MAR), that is, \\(\\boldsymbol{O}\\) and \\(\\boldsymbol{X}\\) are statistically independent and that the distribution of \\(\\boldsymbol{O}\\) does not depend on the hyperparameters. Let \\(\\boldsymbol{X}^{obs} = \\{X_{ijl} : O_{ijl} = 1\\}\\) and \\(\\boldsymbol{X}^{miss} = \\{X_{ijl} : O_{ijl} = 0\\}\\). Let \\(\\boldsymbol{\\Theta}= \\{a, b, \\vartheta, \\sigma\\}\\). We obtain the following expression by integrating out the missing attributes \\[\\begin{equation*} \\small \\begin{split} &amp; p(\\boldsymbol{\\Lambda}, \\boldsymbol{Y}, \\boldsymbol{z}, \\boldsymbol{\\beta}, \\boldsymbol{\\Theta}, \\boldsymbol{X}^{miss} | \\boldsymbol{X}^{obs}, \\boldsymbol{O}) \\\\ &amp; \\propto p(\\boldsymbol{\\beta}| \\boldsymbol{\\Theta}) \\times p(\\boldsymbol{z}| \\boldsymbol{\\beta}, \\boldsymbol{\\Theta}) \\times p(\\boldsymbol{\\Lambda}| \\boldsymbol{\\Theta}) \\times p(\\boldsymbol{Y}) \\\\ &amp; \\quad \\times p(\\boldsymbol{X}^{miss} | \\boldsymbol{\\Lambda}, \\boldsymbol{Y}, \\boldsymbol{z}) \\times p(\\boldsymbol{X}^{obs} | \\boldsymbol{\\Lambda}, \\boldsymbol{Y}, \\boldsymbol{z}) \\\\ &amp; \\propto \\left[ \\prod\\limits_i^{D} \\prod\\limits_{\\ell}^{p_s+p_c} \\beta_{i\\ell}^{a-1} (1-\\beta_{i\\ell})^{b-1} \\right] \\times \\left[ \\prod\\limits_i^{D} \\prod\\limits_j^{R_i} \\prod\\limits_{\\ell}^{p_s+p_c} \\beta_{i \\ell}^{z_{ij\\ell}} (1-\\beta_{i\\ell})^{1-z_{ij\\ell}} \\right] \\times \\left[ \\prod\\limits_{j&#39;}^{N} \\prod\\limits_{\\ell}^{p_s+p_c} \\alpha_{\\ell} (Y_{j&#39;\\ell}) \\right] \\\\ &amp; \\quad \\times \\left[ \\prod\\limits_{ij\\ell \\text{ s.t. } O_{ij\\ell}=1} z_{ij\\ell} \\cdot \\phi(X_{ij\\ell} | Y_{\\lambda_{ij}\\ell}) + (1-z_{ij\\ell}) I(X_{ij\\ell} = Y_{\\lambda_{ij}\\ell}) \\right] \\\\ &amp; \\quad \\times \\left[ \\prod\\limits_i^{D} \\prod\\limits_j^{R_i} I(\\lambda_{ij} = \\text{&quot;new&quot;}) \\frac{k_{ij}\\sigma + \\vartheta}{N_{ij} + \\vartheta} + I(\\lambda_{ij} = j&#39;_g \\in \\{j&#39;_1, ..., j&#39;_{k_{i,j-1}}\\}) \\frac{n_g - \\sigma}{N_{ij} + \\vartheta} \\right]. \\end{split} \\tag{3.1} \\end{equation*}\\] We now derive the full conditional distribution of \\(\\pmb{\\Lambda}\\) under the PYP prior: \\[\\begin{equation*} \\small \\begin{split} &amp; p(\\pmb{\\Lambda}| \\boldsymbol{Y}, \\boldsymbol{z}, \\boldsymbol{\\beta}, \\boldsymbol{X}^{miss}, \\boldsymbol{X}^{obs}) \\\\ &amp; \\propto p(\\boldsymbol{\\Lambda}| \\boldsymbol{\\Theta}) \\times p(\\boldsymbol{X}^{obs} | \\boldsymbol{\\Lambda}, \\boldsymbol{Y}, \\boldsymbol{z}) \\\\ &amp; \\propto \\left[ \\prod\\limits_i^{D} \\prod\\limits_j^{R_i} I(\\lambda_{ij} = \\text{&quot;new&quot;}) \\frac{k_{ij}\\sigma + \\vartheta}{N_{ij} + \\vartheta} + I(\\lambda_{ij} = j&#39;_g \\in \\{j&#39;_1, ..., j&#39;_{k_{i,j-1}}\\}) \\frac{n_g - \\sigma}{N_{ij} + \\vartheta} \\right] \\\\ &amp; \\quad \\times \\left[ \\prod\\limits_{ij\\ell \\text{ s.t. } O_{ij\\ell}=1} (1-z_{ij\\ell}) \\delta(Y_{\\lambda_{ij}\\ell}) + z_{ij\\ell} \\cdot \\phi(X_{ij\\ell} | Y_{\\lambda_{ij}\\ell}) \\right]. \\end{split} \\end{equation*}\\] The full conditional distributions for other parameters remain the same as in and we show them in Appendix . The full conditional distributions under the DP framework is only different from that under the PYP framework in the cluster assignment probabilities \\(P(\\lambda_{ij} \\sim H_0 | \\lambda_{1,1}, ..., \\lambda_{i, j-1}, \\vartheta, H_0)\\) and \\(P(\\lambda_{ij} = j&#39;_g \\in \\{j&#39;_1, ..., j&#39;_{k_{i,j-1}}\\}| \\lambda_{1,1}, ..., \\lambda_{i, j-1}, \\vartheta, H_0)\\). "],
["synthetic.html", "Chapter 4 Application To Synthetic Data 4.1 Experiments 4.2 Discussion of Application on RLdata500", " Chapter 4 Application To Synthetic Data 4.1 Experiments 4.2 Discussion of Application on RLdata500 In this section, we briefly summarize our results on the RLdata500 data set. Under the best settings described in Section 4.1, the PYP prior overall performs the best in terms of the precision (1), recall (0.98) with a 95 percent credible interval of [451, 457]. There is a tendency for the PYP to slightly overestimate the true value of number of unique records, whereas the DP and Uniform prior consistently underestimate the true value. All methods tend to have a balance between the recall and precision, which is a desired property in the field of entity resolution. In terms of computational complexity, the uniform prior is the slowest to mix, which is not unexpected given the fact that we are starting from a very unlikely configuration of the linkage structure. This results in the Gibbs sampler to need to be run for much longer compared to the PYP or DP prior. For example, if we have some idea about the true number of clusters of the existing population apriori, we are able to provide guidance regarding the two unknown parameters of the PYP prior. We are able to provide similar guidance for the DP prior. This in turn allows for faster mixing and a faster computational time of the Gibbs sampler. "],
["elsalvador.html", "Chapter 5 Application to El Salvodoran Conflict Data 5.1 Experiments 5.2 Discussion of Application on UNTC data", " Chapter 5 Application to El Salvodoran Conflict Data 5.1 Experiments 5.2 Discussion of Application on UNTC data "],
["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion In this paper, we have provided five novel contributions to the literature. First, we have introduced to our knowledge, the first subjective priors (the Pitman Yor and Dirichlet Process Priors) for entity resolution with both categorical and string valued data. Second, we have introduced missing values into our model, making the model more realistic to real data situations. Third, we have derived the conditional distributions and implemented a Gibbs sampler for our proposed model. Fourth, we have illustrated the strength and weakness of our model on both synthetic and real data. For the synthetic data, our model performs better than the uniform prior, where performance is measure by standard entity resolution comparisons. For the real data (UNTC data set), our model does well with respect to inference of the underlying population here, however, the precision and recall suffer. Perhaps there may be better similarity metrics that might work to adapt to how names appear in this dataset here, however, this seems to be a difficult task. It seems a centroid or latent variable model may not be the best approach for such data, and this is still under exploration and left for future work. "],
["the-first-appendix.html", "A The First Appendix", " A The First Appendix This first appendix includes all of the R chunks of code that were hidden throughout the document (using the include = FALSE chunk tag) to help with readibility and/or setup. In the main Rmd file # This chunk ensures that the thesisdowndss package is # installed and loaded. This thesisdowndss package includes # the template files for the thesis. library(thesisdowndss) library(servr) In Chapter ??: "],
["the-second-appendix-for-fun.html", "B The Second Appendix, for Fun", " B The Second Appendix, for Fun "],
["references.html", "References", " References "]
]
